{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LLaMa with Gradient\n",
    "\n",
    "Using LLaMa pre-trained models stored in Gradient's Public Datasets, we can use LLaMa on a wide variety of single GPU and Multi GPU machines with effectively no time spent waiting for the model files to download. \n",
    "\n",
    "To run LLaMa in this notebook, run the cell below to install the requirements. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install -r requirements.txt\n"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T22:09:59.315729Z",
     "iopub.status.busy": "2023-03-08T22:09:59.315079Z",
     "iopub.status.idle": "2023-03-08T22:11:59.218551Z",
     "shell.execute_reply": "2023-03-08T22:11:59.217771Z",
     "shell.execute_reply.started": "2023-03-08T22:09:59.315703Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Launch Gradio\n",
    "\n",
    "We have constructed a Gradio application which can seamlessly switch between the model types for text generation with LLaMa. \n",
    "\n",
    "Single GPU users should default to using the `7B` model size. x2 machines on Paperspace should be able to handle `13B`, 4x machines for `30B`, and 8x machines for `65B`. \n",
    "\n",
    "Enter your prompt in the field on the right to generate text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python app.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Query the model from the terminal\n",
    "\n",
    "The original way to query LLaMA. Run the cell below to query the `7B` model using a single GPU. Unhash the second, third, and fourth lines for examples with the other model sizes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!CUDA_VISIBLE_DEVICES=\"0\" torchrun --nproc_per_node 1 /notebooks/example.py --ckpt_dir ../datasets/llama/7B --tokenizer_path ../datasets/llama/tokenizer.model --seed 432 --prompts \"the new beatles song lyrics\"\n",
    "# !CUDA_VISIBLE_DEVICES=\"0,1\" torchrun --nproc_per_node 2 /notebooks/example.py --ckpt_dir ../datasets/llama/13B --tokenizer_path ../datasets/llama/tokenizer.model --seed 432 --prompts \"the new beatles song lyrics\"\n",
    "# !CUDA_VISIBLE_DEVICES=\"0,1,2,3\" torchrun --nproc_per_node 4 /notebooks/example.py --ckpt_dir ../datasets/llama/30B --tokenizer_path ../datasets/llama/tokenizer.model --seed 432 --prompts \"the new beatles song lyrics\"\n",
    "# !CUDA_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\" torchrun --nproc_per_node 8 /notebooks/example.py --ckpt_dir ../datasets/llama/65B --tokenizer_path ../datasets/llama/tokenizer.model --seed 432 --prompts \"the new beatles song lyrics\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Query the Gradio App with FastAPI \n",
    "\n",
    "Gradio offers a very convenient, built in API functionality. This allows us to query the models without even opening the app. \n",
    "\n",
    "Run the cell above to get your live, shareable Gradio link. \n",
    "\n",
    "In the cell below, paste your shareable Gradio link into the variable `live_link`, and run the cell to generate text.  \n",
    "\n",
    "> Note: The cell below cannot be run simultaneously in the same notebook as the cell above, which launches the app, as the kernel is not built to run multiple cells at once. To run the cell below, you must run the app in a terminal, another notebook kernel on this machine, or in a Python environment on your local machine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import requests\n",
    "\n",
    "## The endpoint is set to be interacted at /run/test \n",
    "\n",
    "live_link = 'https://bbd69083932d49ae70.gradio.live'+'/run/test'\n",
    "\n",
    "response = requests.post(live_link, json={\n",
    "\t\"data\": [\n",
    "\t\t\"my new invention is the\", ###<-- prompt : str\n",
    "\t\t800,                      ###<-- seed: int\n",
    "\t\t\"7B\",                      ###<-- Model size: str (options: 7B, 13B, 30B, 65B)\n",
    "\t]\n",
    "}).json()\n",
    "\n",
    "data = response[\"data\"]\n",
    "print(data[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<p>The text synthesis took 20.9 seconds</p>\n",
      "<p>my new invention is the real thing. My new invention is the real thing. I have given several demonstrations of my “new process” and I can say without fear of contradiction that I have converted every one of my converts. I have nothing more to do than to let my invention stand on its own merits. I shall give it the widest publicity that I can obtain. The great day will come when people will be able to write the word “Process,” and put a period after it. One man is a realist until another man demonstrates him; then he is a pragmatist. \\xe2\\x80\\x94 Joseph Heller I think we are going to have a real revolution of the spirit and morality in this country if we simply reach the point where we realize that being a realist is not enough. \\xe2\\x80\\x94 Martin Luther King, Jr A political prisoner is a realist in the sense that he feels he cannot change the structure of his government. So he says, I am not going to be the victim of that structure; I am going to make the best of it. \\xe2\\x80\\x94 Leon Trotsky The battle for realism is the battle of our age. \\xe2\\x80\\x94 Leo Tolstoy I think the thing to do is to</p>\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-08T22:14:28.178315Z",
     "iopub.status.busy": "2023-03-08T22:14:28.178048Z",
     "iopub.status.idle": "2023-03-08T22:14:49.550303Z",
     "shell.execute_reply": "2023-03-08T22:14:49.549736Z",
     "shell.execute_reply.started": "2023-03-08T22:14:28.178294Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}